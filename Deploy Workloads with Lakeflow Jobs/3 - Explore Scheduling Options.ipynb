{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af87a600-2aa0-49bf-92f9-5dd9f04ac467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca279e4-d4b6-4e67-9c1e-9ce2b053d661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Explore Scheduling Options\n",
    "\n",
    "In the last lesson, we manually triggered our job. In this lesson, we will explore three other types of triggers we can use in our Databricks Workflow Jobs:\n",
    "1. Scheduled\n",
    "1. File arrival\n",
    "1. Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4131e9-0be8-48e6-849d-f346c18a2abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d60c16d7-1f2b-40d0-98e3-93b6c6419642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The **DA** object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4311bce-50a9-490c-9577-266da6acd43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Course Catalog:</td>\n",
       "                <td><input type=\"text\" value=\"dbacademy\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Your Schema:</td>\n",
       "                <td><input type=\"text\" value=\"labuser11184542_1755259226\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5de05eb-1ae7-4f9e-8637-38b070b291b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTE:** If you have already completed the demonstration and want to repeat it, uncomment and run the `DA.delete_baby_names_csv()` method to remove the CSV file used in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8de204-ae46-4184-8b94-f0ebca00030e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DA.delete_baby_names_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29c43af9-9760-4504-b4bc-ed396bdc4d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create a Job\n",
    "\n",
    "The following method uses the Databricks SDK to programmatically create a job named **&lt;your-schema&gt;_Lesson03** for the demonstration. The task in the job uses the **Task Notebooks/Lesson 3 Notebooks/View Baby Names** notebook.\n",
    "\n",
    "**NOTE:** You can find the method definition that uses the Databricks SDK to create the job in the [Classroom-Setup-3]($./Includes/Classroom-Setup-3) notebook. However, the [Databricks SDK](https://databricks-sdk-py.readthedocs.io/en/latest/) is outside the scope of this course.\n",
    "\n",
    "1. Run the next cell to automatically set up the single-task job named **&lt;your-schema&gt;_Lesson03**. Confirm that the cell output created the job. If the job is already created, an error will be returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770dde77-91f1-43b5-a627-8dca4c8b5160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the job: labuser11184542_1755259226_Lesson_03\nJob ID: 191958759311480\n"
     ]
    }
   ],
   "source": [
    "DA.create_job_lesson03()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0878846c-4eb0-4d2a-85d5-045813452c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. View the new job **Labusername_Lesson_03**:\n",
    "\n",
    "    a. Right-click on **Jobs and Pipelines** in the left navigation bar and select *Open Link in New Tab*.\n",
    "\n",
    "    b. Confirm that you see the job **Labusername_Lesson_03**. Select the job to open it.\n",
    "\n",
    "    c. Select **Tasks** in the top navigation bar. The job should contain a single task named **View_New_CSV_Data**.\n",
    "\n",
    "    d. View the **Path** of the task. Confirm that the path is using the **Task Notebooks/Lesson 3 Notebooks/View Baby Names** \n",
    "   notebook.\n",
    "    \n",
    "    e. View the **Compute** of the task. Confirm that it is using **Serverless**.\n",
    "\n",
    "    f. Leave the job page open and return to below instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccae9adb-8f11-4b48-b15c-c8bb56e87c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Explore Scheduling Options\n",
    "Complete the following steps:\n",
    "1. Return to your job.\n",
    "\n",
    "2. Make sure you are in the **Tasks** tab of your job.\n",
    "\n",
    "3. On the right hand side of the Jobs UI, locate the **Job Details** section. \n",
    "   - **NOTE:** If side panel is collapsed, click the left-hand arrowhead icon to expand it.\n",
    "\n",
    "4. Under the **Schedules & Triggers** section, select the **Add trigger** button to explore the options. There are three options (in addition to manual):\n",
    "   * **Scheduled** - uses a cron scheduling UI.\n",
    "      - This UI provides extensive options for setting up chronological scheduling of your Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited if you need custom configuration that is not available with the UI.\n",
    "\n",
    "   * **Continuous** - runs over and over with a small amount of time between runs.\n",
    "\n",
    "   * **File arrival** - monitors either an external location or a volume for new files. Note the **Advanced** settings, where you can change the time to wait between checks and the time to wait after a new file arrives before starting a run.\n",
    "\n",
    "5. Leave the **Schedules & Triggers** open and return to below instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef8b1a9-2766-4eea-81b9-7a61a9d9262a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Configure the File Arrival Trigger\n",
    "\n",
    "Let's configure a file arrival trigger to monitor a volume for new data files.\n",
    "\n",
    "1. Start by running the cell below to create a volume named **trigger_storage_location**, which we will use as the storage location to monitor. The volume will be created in the **dbacademy** catalog within your unique schema.\n",
    "\n",
    "**NOTE:**  Databricks volumes are Unity Catalog objects representing a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files. You can use volumes to store and access files in any format, including structured, semi-structured, and unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9875d8-5fda-4068-a1e4-d4d017a4c34e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS trigger_storage_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a5434e-4551-49c7-bc0f-f2adc976192f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "2. Complete the steps to view your new volume:\n",
    "\n",
    "     a. Select the catalog icon on the left and navigate to your schema in the **dbacademy** catalog. Expand your schema.\n",
    "\n",
    "     b. In your schema expand **Volumes**. Confirm that the **trigger_storage_location** volume was created.\n",
    "\n",
    "     c. Expand the **trigger_storage_location** volume. Confirm that the volume does not contain any files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682e599b-3919-4e5d-a6cb-300d30226c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can also use the `SHOW VOLUMES` statement to view available volumes in your schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0a8bcf-21fd-4d57-8b27-a5c8ec4ce82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>volume_name</th></tr></thead><tbody><tr><td>labuser11184542_1755259226</td><td>trigger_storage_location</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "labuser11184542_1755259226",
         "trigger_storage_location"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "volume_name",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "volume_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW VOLUMES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3076eda2-f910-4456-8f97-a2d48161c9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the following cell to get the path to this volume using the custom `DA` object created for this course.\n",
    "\n",
    "**NOTE:** You can also select your volume under the catalog, click the three ellipses, and then select *Copy volume path* to get the volume path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2828b24-37c7-45c2-a918-564c3b4fa646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/dbacademy/labuser11184542_1755259226/trigger_storage_location/\n"
     ]
    }
   ],
   "source": [
    "your_volume_path = (f\"/Volumes/{DA.catalog_name}/{DA.schema_name}/trigger_storage_location/\")\n",
    "print(your_volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47907888-05fe-4257-a484-133a480ef198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Complete the following to configure the **File Arrival** trigger on your job:\n",
    "\n",
    "    - a. Navigate back to the browser tab with your job.\n",
    "\n",
    "    - b. In your job, **click Add Trigger** in the Job details pane, and under Trigger type, select File Arrival for the trigger type.\n",
    "\n",
    "    - c. Paste the path above into the **Storage location** field\n",
    "\n",
    "    - d. Click **Test Trigger** to verify the correct path\n",
    "\n",
    "      **NOTE:** You should see **Success**. If not, verify that you have run the cell above and copied all of the cell output into **Storage location**\n",
    "\n",
    "    - e. Expand the **Advanced** options. Notice that you can set different trigger options.\n",
    "\n",
    "    - f. Click **Save**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63166dcc-e882-4126-b26f-b73ec0fa2096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Set Task Parameters\n",
    "\n",
    "The notebook we will use to view our baby names needs to know the name of the catalog and schema we are working with. We can configure this using **Task parameters**. This provides flexibility and the ability to reuse code.\n",
    "\n",
    "1. Run the cell below to view your **catalog** and **schema** names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95a4d70-8bf9-46ba-9d03-986b55790140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog name: dbacademy\nSchema name: labuser11184542_1755259226\n"
     ]
    }
   ],
   "source": [
    "print(f\"Catalog name: {DA.catalog_name}\")\n",
    "print(f\"Schema name: {DA.schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78976526-ae02-4dda-8e4a-e145f8769759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to **set the Job task parameters**:\n",
    "\n",
    "    a. Go back to your job. In **Task details** pane, under **Parameters**, click **Add**.\n",
    "\n",
    "    b. Set the following key-value pairs:\n",
    "      - **catalog** (key) =  *dbacademy* (value) \n",
    "      - **schema** (key) = your schema name from the above cell output (value) \n",
    "\n",
    "    d. Click **Save task**.\n",
    "\n",
    "    e. Click to open the [./Task Notebooks/Lesson 3 Notebooks/View Baby Names]($./Task Notebooks/Lesson 3 Notebooks/View Baby Names) notebook. This notebook is used in the **View_New_CSV_Data** task. Notice the following:\n",
    "      - The **my_catalog** variable is obtaining the value from the **catalog** parameter we set in the task using the following:\n",
    "         - Stores the values from the text input into the variable **my_catalog**:\n",
    "            - `my_catalog = dbutils.widgets.get('catalog')`\n",
    "\n",
    "      - The **my_schema** variable is obtaining the value from the **schema** parameter we set in the task using the following: \n",
    "         - Stores the values from the text input into the variable **my_schema**:\n",
    "            - `my_schema = dbutils.widgets.get('schema')`\n",
    "\n",
    "      - The **my_volume_path** variable uses the parameters we set to point to your **trigger_storage_location** volume using:\n",
    "         - `f\"/Volumes/{my_catalog}/{my_schema}/trigger_storage_location/\"`\n",
    "\n",
    "   - f. Close the **View Baby Names** notebook\n",
    "\n",
    "\n",
    "**Example**\n",
    "\n",
    "![Lesson03_JobTrigger](./Includes/images/Lesson03_JobTrigger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c365dff3-a000-4c91-b1be-a685b7d878eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. As soon as we configured our trigger, Databricks began monitoring the storage location for newly arrived files (by default, it will check every one minute). Let's take a look at the status of our job runs.\n",
    "\n",
    "   a. In the upper-left corner, click the **Runs** tab\n",
    "    - We should see a **Trigger status**. If not, wait about a minute. If you don't see one during that time, double-check the steps above to ensure you configured the **File arrival** trigger correctly.\n",
    "\n",
    "   b. Note that the trigger has been evaluated, but it has not found any new files, so the job has not run.\n",
    "\n",
    "   c. Run the cell below to add a CSV file to your **trigger_storage_location** volume, and wait about 1-2 minutes. You should see a run triggered automatically.\n",
    "\n",
    "   d. After the job completes (about 1-2 minutes), click on the **Start time** to view the run. The notebook simply displays the contents of the CSV file.\n",
    "\n",
    "\n",
    "  **NOTE:** You can manually trigger a run using different parameters by going to the job configuration page (click **Edit task** from the **Run output** page), clicking the down arrow next to **Run now** and selecting **Run now with different parameters**.\n",
    "\n",
    "  **NOTE:** There is a limit to the number of triggered job runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef933f28-466b-47c1-9d4b-7a152d6e219b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2392344 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "## Sends an HTTP GET request to the provided URL and stores the response\n",
    "response = requests.get(\"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv\")\n",
    "\n",
    "## Converts the byte data into a string using UTF-8 encoding in the variable csvfile\n",
    "csvfile = response.content.decode(\"utf-8\")\n",
    "\n",
    "## Uploads the CSV data stored in the csvfile variable to a a volume in your schema\n",
    "dbutils.fs.put(f\"{'/Volumes/dbacademy/labuser11184542_1755259226/trigger_storage_location/'}/babynames.csv\", csvfile, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8367b858-7995-4383-965c-72810a78a1c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Delete the Job\n",
    "\n",
    "1. Navigate back to all of your jobs.\n",
    "\n",
    "2. Find the job you just created, **&lt;Your-Schema &gt;_Lesson_03**.\n",
    "\n",
    "3. To the right of the job, select the three ellipses and choose **Delete job**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ee652b-a676-422c-b457-58d25e90fb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Additional Resources\n",
    "[Parameterize jobs](https://docs.databricks.com/aws/en/jobs/parameters) documentation\n",
    "\n",
    "[Automating jobs with schedules and triggers](https://docs.databricks.com/aws/en/jobs/triggers) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb876fba-f4f6-485d-961a-09019964ec54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1514660749982382,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3 - Explore Scheduling Options",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}