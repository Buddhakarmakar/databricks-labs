{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7acf5e-8c5a-4431-8bc7-80dd94577664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e9b339-ac4b-4a79-bc1c-5f97cdaf0ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 Lab - Create a Job with Multiple Tasks\n",
    "### Duration: ~15 minutes\n",
    "\n",
    "\n",
    "In this lab, you'll be configuring a multi-task job comprising of three notebooks.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you should be able to:\n",
    "* Schedule a notebook as a task in a Databricks Job\n",
    "* Configure linear dependencies between tasks using the Databricks Workflows UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd447015-6100-46f5-b24e-268b57b84e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The **DA** object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91de578-d176-467b-8902-f7632ae6d602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e218c2-df8c-4e6a-99fd-ed3132d75eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Course Catalog:</td>\n",
       "                <td><input type=\"text\" value=\"dbacademy\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Your Schema:</td>\n",
       "                <td><input type=\"text\" value=\"labuser11184542_1755240991\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-2L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861c3354-c01e-47fc-99c5-96cff7093a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Generate Job Configuration\n",
    "1. Run the cell below to print out the values you'll use to configure your pipeline in subsequent steps. Make sure to specify the correct job name and notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cd53d9-bfa4-40e4-b8e5-e771797a337e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
       "                <td><input type=\"text\" value=\"labuser11184542_1755240991_Lesson_02\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #1:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser11184542_1755240991@vocareum.com/deploy-workloads-with-lakeflow-jobs-3.0.1/Deploy Workloads with Lakeflow Jobs/Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #2:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser11184542_1755240991@vocareum.com/deploy-workloads-with-lakeflow-jobs-3.0.1/Deploy Workloads with Lakeflow Jobs/Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table\" style=\"width: 100%\"></td></tr>\n",
       "            <tr>\n",
       "                <td style=\"white-space:nowrap; width:1em\">Notebook #3:</td>\n",
       "                <td><input type=\"text\" value=\"/Workspace/Users/labuser11184542_1755240991@vocareum.com/deploy-workloads-with-lakeflow-jobs-3.0.1/Deploy Workloads with Lakeflow Jobs/Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table\" style=\"width: 100%\"></td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.print_job_config(\n",
    "    job_name_extension='Lesson_02',\n",
    "    notebook_paths='/Task Notebooks/Lesson 2 Notebooks',\n",
    "    notebooks=[\n",
    "        '2.01 - Ingest CSV',\n",
    "        '2.02 - Create Invalid Region Table',\n",
    "        '2.02 - Create Valid Region Table'\n",
    "    ],\n",
    "    job_tasks={\n",
    "        'Ingest_CSV': [],\n",
    "        'Create_Invalid_Region_Table': ['Ingest_CSV'],\n",
    "        'Create_Valid_Region_Table': ['Ingest_CSV']\n",
    "    },\n",
    "    check_task_dependencies = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58913183-8408-4583-b2b5-fb64dace0fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Configure a Job With Multiple Tasks\n",
    "The job will complete three simple tasks:\n",
    "\n",
    "1. (Notebook #1) Ingest a CSV file and create the **customers_bronze** table in your schema.\n",
    "2. (Notebook #2) Create a table called **customers_invalid_region** in your schema.\n",
    "3. (Notebook #3) Create a table called **customers_valid_region** in your schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8996c08-2b1a-45a2-a2c5-d18bccb88d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Add a Single Notebook Task\n",
    "\n",
    "Let's start by scheduling the first notebook [./Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV]($./Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV) notebook. Click the hotlink in previous sentence to review the code.\n",
    "\n",
    "The notebook creates a table named **customers_bronze** in your schema from the CSV file in the volume */Volumes/dbacademy_retail/v01/source_files/customers.csv*. \n",
    "\n",
    "1. Right click on the **Jobs and Pipelines** button on the sidebar and select *Open Link in New Tab*. \n",
    "\n",
    "2. Select the **Jobs & Pipeline** tab, and then click the **Create** button and **choose Job from the dropdown**.\n",
    "\n",
    "3. In the top-left of the screen, enter the **Job Name** provided above to add a name for the job (must use the job name specified above).\n",
    "\n",
    "4. Configure the task as specified below. You'll need the values provided in the cell output above for this step.\n",
    "\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Ingest_CSV** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #1** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.01 - Ingest CSV**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "\n",
    "**NOTE**: When selecting your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate.\n",
    "<br>\n",
    "\n",
    "![Job Demo 1](./Includes/images/Lesson02_Lab_OneTask.png)\n",
    "\n",
    "5. Click the **Create task** button.\n",
    "\n",
    "6. Click the blue **Run now** button in the top right to start the job.\n",
    "\n",
    "7. Select the **Runs** tab in the navigation bar and verify that the job completes successfully.\n",
    "\n",
    "![Job Demo 1](./Includes/images/Lesson02_Lab_OneTaskSuccess.png)\n",
    "\n",
    "8. From **Catalog**, navigate to your schema in the **dbacademy** catalog and confirm the table **customers_bronze** was created (you might have refresh your schema)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03a67b2-4b34-40b8-91cf-ee5c834ba2c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Add the Second Task to the Job\n",
    "\n",
    "Now, configure a second task that depends on the first task, **Ingest_CSV** successfully completing. The second task will be the notebook [./Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Table]($./Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table). Open the notebook and review the code.\n",
    "\n",
    "The notebook creates a table named **customers_invalid_region** in your schema from the **customers_bronze** table created from the previous task.\n",
    "\n",
    "Steps:\n",
    "1. Go back to your job. On the Job details page, click the **Tasks** tab.\n",
    "\n",
    "2. Click the blue **+ Add task** button at the center bottom of the screen and select **Notebook** in the dropdown menu.\n",
    "\n",
    "3. Configure the task:\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Create_Invalid_Region_Table** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #2** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.02 - Create Invalid Region Table**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "| Depends on | Verify **Ingest_CSV** (the previous task we defined) is listed |\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Click the blue **Create task** button\n",
    "\n",
    "<br></br>\n",
    "![Job Demo 1](./Includes/images/Lesson02_Lab_TwoTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f5c5c7-9619-4c99-9ec6-9f6bfef65878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Add the Third Task to the Job\n",
    "\n",
    "Now, configure a third task that depends on the **Ingest_CSV** successfully completing. The third task will be the notebook [./Task Notebooks/Lesson 2 Notebooks/2.03 - Create Valid Table]($./Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table). \n",
    "\n",
    "The notebook creates a table named **customers_valid_region** in your schema from the **customers_bronze** table created from the first task.\n",
    "\n",
    "Steps:\n",
    "1. On the Job details page, confirm you are on the **Tasks** tab.\n",
    "\n",
    "2. Click on the **Ingest_CSV** tasks.\n",
    "\n",
    "3. Click the blue **+ Add task** button at the center bottom of the screen and select **Notebook** in the dropdown menu.\n",
    "\n",
    "4. Configure the task:\n",
    "\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Create_Valid_Region_Table** |\n",
    "| Type | Choose **Notebook** |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Notebook #3** path provided above (notebook **Task Notebooks/Lesson 2 Notebooks/2.02 - Create Valid Region Table**) |\n",
    "| Compute | From the dropdown menu, select a **Serverless** cluster (We will be using Serverless clusters for jobs in this course. You can also specify a different cluster if required outside of this course) |\n",
    "| Depends on | Remove current **Depends on** task and replace with **Ingest_CSV** (the previous task we defined) is listed |\n",
    "\n",
    "5. Click the blue **Create task** button\n",
    "\n",
    "<br></br>\n",
    "![Job Demo 1](./Includes/images/Lesson02_Lab_ThreeTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e23609-c984-4077-b9e7-8c007ba1d50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Verify the Job is Configured Correctly\n",
    "Run the cell below to check if you configured the job correctly. Modify any errors. \n",
    "\n",
    "**NOTE:** Errors include naming inconsistencies. If an error is returned and the job is setup it correctly it still should run. Feel free to try it and view the job run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c43553-8cb7-4db8-b19b-b32b952418c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Required job Id has been found.\n2. Required job name labuser11184542_1755240991_Lesson_02 has been found.\n3. Required task notebooks set correctly.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-37179099377767>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m DA\u001B[38;5;241m.\u001B[39mvalidate_job_config()\n",
       "\n",
       "File \u001B[0;32m<command-37179099377807>, line 17\u001B[0m, in \u001B[0;36mvalidate_job_config\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_job_notebooks()\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m## Test task names in job\u001B[39;00m\n",
       "\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_job_tasks()\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m## If all tests pass, print the message.\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-------------------------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-37179099377806>, line 38\u001B[0m, in \u001B[0;36mvalidate_job_tasks\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     37\u001B[0m     name_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m name_not_found, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly.\u001B[39m\u001B[38;5;124m'\u001B[39m \n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m##\u001B[39;00m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m## Check task dependencies\u001B[39;00m\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m##\u001B[39;00m\n",
       "\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_task_dependencies \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: The specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AssertionError",
        "evalue": "The specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: The specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-37179099377767>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m DA\u001B[38;5;241m.\u001B[39mvalidate_job_config()\n",
        "File \u001B[0;32m<command-37179099377807>, line 17\u001B[0m, in \u001B[0;36mvalidate_job_config\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_job_notebooks()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m## Test task names in job\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_job_tasks()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m## If all tests pass, print the message.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-------------------------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m<command-37179099377806>, line 38\u001B[0m, in \u001B[0;36mvalidate_job_tasks\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     37\u001B[0m     name_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m name_not_found, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly.\u001B[39m\u001B[38;5;124m'\u001B[39m \n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m##\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m## Check task dependencies\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m##\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_task_dependencies \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
        "\u001B[0;31mAssertionError\u001B[0m: The specified job tasks names were not set correctly. Please review the notebook directions and name the tasks correctly."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.validate_job_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4d7d1f-f4ff-4a28-9bb4-be5a2984813e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Run the Job\n",
    "1. Click the blue **Run now** button in the top right to run this job. It should take a few minutes to complete.\n",
    "\n",
    "2. From the **Runs** tab, you will be able to click on the start time for this run under the **Active runs** section and visually track task progress.\n",
    "\n",
    "3. On the **Runs** tab confirm that the job completed successfully.\n",
    "\n",
    "<br></br>\n",
    "![Job Demo 1](./Includes/images/Lesson02_Lab_SuccessRun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c77df1-f580-44b6-829a-b2ab3d49433d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. View the New Tables\n",
    "1. In the left pane, select **Catalog**.\n",
    "\n",
    "2. Expand the **dbacademy** catalog.\n",
    "\n",
    "3. Expand your unique schema name.\n",
    "\n",
    "4. Confirm that the job created the following tables:\n",
    "  - **customers_bronze**\n",
    "  - **customers_invalid_region**\n",
    "  - **customers_valid_region**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605fc94c-8eef-43d3-b521-d6419b96b2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can also use the `SHOW TABLES` statement to view available tables in your schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982d94a7-97b1-49d4-adfc-12531511f36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>labuser11184542_1755240991</td><td>customers_bronze</td><td>false</td></tr><tr><td>labuser11184542_1755240991</td><td>customers_invalid_region</td><td>false</td></tr><tr><td>labuser11184542_1755240991</td><td>customers_valid_region</td><td>false</td></tr><tr><td>labuser11184542_1755240991</td><td>my_user_table</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "labuser11184542_1755240991",
         "customers_bronze",
         false
        ],
        [
         "labuser11184542_1755240991",
         "customers_invalid_region",
         false
        ],
        [
         "labuser11184542_1755240991",
         "customers_valid_region",
         false
        ],
        [
         "labuser11184542_1755240991",
         "my_user_table",
         false
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tableName",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isTemporary",
            "nullable": false,
            "type": "boolean"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8cb333-e21f-4763-904d-02f5852a48ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 37179099377771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2 Lab - Create a Job with Multiple Tasks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}